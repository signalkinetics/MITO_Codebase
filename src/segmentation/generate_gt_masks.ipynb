{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ground Truth Mask Generation\n",
    "This tool can be used to manually generate ground-truth masks. It is based on the SAM model and behaves similarly to their online demo. An iteractive plot will be shown, and you can add new prompt points by clicking on the image. The image will then be updated with the new SAM masks. Currently, this tool supports the following features:\n",
    "\n",
    "- Left click to add a positive prompt point\n",
    "- Right click to add a negative prompt point\n",
    "- Center click (on a mouse) to remove the nearest point from the list of prompts\n",
    "- Use the mouse scroll wheel to zoom in on the image\n",
    "- You can choose to segment either RGB images or mmWave images\n",
    "\n",
    "We have already provided ground-truth segmentations of the RGB images and some mmWave images (created using this tool). \n",
    "\n",
    "\n",
    "To begin, start running the code cells below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This notebook is based on the SAM online demo and example notebook (https://github.com/facebookresearch/segment-anything/blob/main/notebooks/onnx_model_example.ipynb)\n",
    "# This cell imports required packages\n",
    "\n",
    "%matplotlib widget \n",
    "# Install ipympl https://github.com/matplotlib/ipympl\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_point_clicker import clicker\n",
    "from matplotlib.backend_bases import MouseButton\n",
    "from mpl_interactions import ioff, zoom_factory\n",
    "import requests\n",
    "import onnxruntime\n",
    "from onnxruntime.quantization import QuantType\n",
    "from onnxruntime.quantization.quantize import quantize_dynamic\n",
    "import os\n",
    "import sys\n",
    "import copy\n",
    "\n",
    "from segment_anything import sam_model_registry, SamPredictor\n",
    "from segment_anything.utils.onnx import SamOnnxModel\n",
    "\n",
    "sys.path.append('..')\n",
    "from segmentation_utilities import *\n",
    "from utils.utilities import *\n",
    "from utils.object_information import ObjectInformation\n",
    "from utils.generic_loader import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up the SAM Model\n",
    "Next, we will set up the SAM model. If the weights are not found, we will download them for you. This may take a few minutes. If you have SAM weights downloaded elsewhere on your computer, change the path in utils/params.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell will check if the SAM weights exist. If they don't, it download the weights. \n",
    "# By default, this notebook looks for weights in the src/segmentation/weights/\n",
    "# If you have SAM weights downloaded elsewhere on your computer, change the path in utils/params.json to use those weights\n",
    "weight_path = check_for_sam_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell creates the SAM model. It shouldn't be re-run after it succeededs, since it will create a duplicate copy of the model\n",
    "model_type = \"vit_h\"\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "sam = sam_model_registry[model_type](checkpoint=weight_path)\n",
    "sam.to(device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the code segments below to begin segmentation! \n",
    "First, use the next cell to select an object. By default, an input prompt will appear that accepts either an object ID number or an object name. \n",
    "\n",
    "Then, the following cell specifies whether to show the RGB or mmWave image for segementation. \n",
    "\n",
    "Finally, the following cell will run the segmentation tool. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Please input either an object ID or name (see top of screen for input prompt on VS code)')\n",
    "time.sleep(1) \n",
    "obj_id_or_name = input(\"Input the id number or name of the object: \")\n",
    "obj_id, obj_name = ObjectInformation().fill_in_identifier(obj_id_or_name)\n",
    "print(f'Starting segmentation tool for object: ({obj_id}, {obj_name})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change these variables to choose which type of image to show (mmWave vs camera, line-of-sight vs non-line-of-sight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# True for RGB image, False for mmWave image\n",
    "use_camera = True\n",
    "\n",
    "# True for line-of-sight image, False for mmWave image. This is ignored if using the camera\n",
    "use_los = True\n",
    "\n",
    "# Which radar image to show (24_ghz or 77_ghz). This is ignored if using the camera\n",
    "radar_type = '77_ghz'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell runs the segmentation tool. An interactive plot will appear below the cell. Left click points (displayed as green stars) will be included in the mask and Right click points (displayed as red stars) will be excluded from the mask. To remove either point from the collection of point, click the center button on the mouse as close as possible to the point you aim to remove. Use the mouse wheel to zoom in/out. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell runs the demo. An interactive plot will appear below the cell. Rerun this cell to restart the segmentation\n",
    "\n",
    "# Load either the RGB or mmWave image\n",
    "loader = GenericLoader(obj_id, obj_name, is_sim=False, is_los=use_los if not use_camera else True)\n",
    "if use_camera:\n",
    "    cam_data = loader.load_camera_data()\n",
    "    image = cam_data['rgb'].astype(np.uint8)\n",
    "else:\n",
    "    ext = utilities.load_param_json()['processing']['robot_collected_extension']\n",
    "    image, locs, antenna_locs = loader.load_image_file(radar_type=radar_type, background_subtraction=None, ext=ext)\n",
    "\n",
    "    # Convert image to 2D by averaging along height\n",
    "    avg_img = np.sum(np.abs(image), axis=2) / image.shape[2]\n",
    "\n",
    "    # Convert image to color by applying colormap\n",
    "    cmap = plt.get_cmap('jet')\n",
    "    color_img = cmap(avg_img)[:,:,:3]\n",
    "    color_img = (color_img * 255).astype(np.uint8)\n",
    "    image = color_img\n",
    "\n",
    "# Set up model with image\n",
    "predictor = SamPredictor(sam)\n",
    "predictor.set_image(image)\n",
    "\n",
    "# Plot an interactive image. The code is based on the example here: https://gist.github.com/jbencina/d1200253ef1ae5d99287f47616687fef\n",
    "with ioff:\n",
    "    fig, ax = plt.subplots()\n",
    "ax.imshow(image)\n",
    "disconnect_zoom = zoom_factory(ax) # Allow zooming in and out of image\n",
    "fig.canvas.toolbar_visible = False\n",
    "fig.canvas.header_visible = False\n",
    "fig.canvas.footer_visible = False\n",
    "ax.axis('off')\n",
    "\n",
    "# Handle events when user clicks on the image\n",
    "all_points = []\n",
    "input_label = []\n",
    "distances = []\n",
    "best_mask_image = None\n",
    "def onclick(event):\n",
    "    # Update mask on each click event\n",
    "    global best_mask_image\n",
    "\n",
    "    # Reset interactive image\n",
    "    ax.cla()\n",
    "    ax.imshow(image)\n",
    "    ax.axis('off')\n",
    "\n",
    "    input_point = np.array([event.xdata, event.ydata]) # Saves the click's coordinate point\n",
    "\n",
    "    # Left Click - Adds a positive prompt point to add segments to the mask\n",
    "    if event.button is MouseButton.LEFT: \n",
    "        # Save click's coordinate as a positive prompt point\n",
    "        all_points.append(input_point)\n",
    "        input_label.append(1) # A label of 1 refers to a positive prompt point\n",
    "\n",
    "    # Right Click - Adds a negative prompt point to remove segments from the mask\n",
    "    if event.button is MouseButton.RIGHT:\n",
    "        # Save click's coordinate as a negative prompt point \n",
    "        all_points.append(input_point)\n",
    "        input_label.append(0) # A label of 0 refers to the negative sectioning \n",
    "\n",
    "    # Center Click - Removes the closest previously selected point\n",
    "    if event.button is MouseButton.MIDDLE: \n",
    "        # Find nearest point to clicked location\n",
    "        distances = []\n",
    "        for point in all_points: \n",
    "            distances.append(math.dist(point, input_point)) \n",
    "        min_ind = np.argmin(distances) \n",
    "\n",
    "        # Remove the point from the current list\n",
    "        all_points.pop(min_ind)\n",
    "        input_label.pop(min_ind)\n",
    "\n",
    "    # Visualize points, call SAM network, and visualize resulting mask\n",
    "    # TODO: Test center click\n",
    "    if len(all_points)!=0:\n",
    "        show_points(np.array(all_points), np.array(input_label), ax)\n",
    "        masks, scores, logits = predictor.predict( # Calls the network \n",
    "            point_coords=np.array(all_points),\n",
    "            point_labels=input_label,\n",
    "            multimask_output=False,\n",
    "        )\n",
    "        torch.cuda.empty_cache() \n",
    "        best_mask_image = masks\n",
    "        show_mask(best_mask_image, ax) # Shows final mask\n",
    "        \n",
    "\n",
    "# Enable interactive canvas\n",
    "cid = fig.canvas.mpl_connect('button_press_event', onclick)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the code below to save the generated masks\n",
    "If this is an RGB mask, it will also create inpainted masks on the corresponding depth images, and corresponding 3D masks. \n",
    "If this is a mmWave mask, it will simply save the generated mask. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell will post-process and save the masks\n",
    "save_masks = True\n",
    "\n",
    "if use_camera:\n",
    "    depth_image = copy.deepcopy(cam_data['depth'])\n",
    "\n",
    "    # Plot original depth image\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.set_title('Original Depth Image')\n",
    "    ax.imshow(depth_image)\n",
    "    plt.show()\n",
    "\n",
    "    # Filter depth image. Remove points that are too close or too far\n",
    "    depth_image[depth_image>500] = np.nan\n",
    "    depth_image[best_mask_image[0,:,:]!=1] = np.nan\n",
    "    depth_image[depth_image<200] = np.nan\n",
    "\n",
    "    # Iteratively inpaint the depth image to fill in nans \n",
    "    for i in range(10):\n",
    "        inpaint_mask = np.zeros_like(depth_image)\n",
    "        inpaint_mask[best_mask_image[0,:,:]==1] = 1\n",
    "        inpaint_mask[depth_image>200] = 0\n",
    "        inpaint_mask = inpaint_mask.astype(np.uint8)\n",
    "        inpainted = cv2.inpaint(depth_image,inpaint_mask,1,cv2.INPAINT_NS)\n",
    "        depth_image = inpainted\n",
    "\n",
    "    # Display the inpainted depth image\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.set_title('Inpainted Masked Depth')\n",
    "    ax.imshow(depth_image)\n",
    "    plt.show()\n",
    "\n",
    "    # Convert inpainted depth to XYZ points\n",
    "    xyz_image = utilities.convert_depth_to_xyz(inpainted)\n",
    "\n",
    "    # Display XYZ points\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.set_title('Inpainted 3D Mask')\n",
    "    ax.imshow(xyz_image)\n",
    "\n",
    "    # Save mask, inpainted image, and 3D mask\n",
    "    if save_masks: loader.save_camera_masks(best_mask_image, inpainted, xyz_image, cam_data, ext='')\n",
    "\n",
    "else:\n",
    "    # Save mask\n",
    "    if save_masks: loader.save_radar_masks(best_mask_image, radar_type, angles=None)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
